Review all my @plans/docs and the code implementation so far. The basic idea of this repo is to provide a single repo that shares the same flake.nix environment to scrape flight data from multiple sources either live or historically (packages/map-scraper, packages/history-scraper), stores the data in a ClickHouse database (clickhouse), exposes the data with graphql resolvers (map-graphql) and finally renders the data with react/deck-gl dynamically based on user filters. Now my code here is VERY experimental and getting bloated. The repo would be done in sprints:

1) I would like to do a minimalistic repository from scratch, focusing on having a clean flake.nix with its latest packages (nixos-25.11) that setsup the latest version of typescript/bun/nodejs/clickhouse 
2) Think deep of the shared interfaces ie. making a shared type for flight_events with locations (timestamp/lat/lon/alt/source/details) and a more elaborate flight_details with whatever https://fr24api.flightradar24.com/docs/sdk/ js and https://openskynetwork.github.io/opensky-api/rest.html provide us with - the shared type should be in a separate package that other repos can import from
3) have a clear data pipeiline from multiple data sources => configurable scraper service => clickhouse ingest table => clickhouse materialized views / enriched data => graphql wrapper => frontend => visualisation
4) smartly index on both time (ie. from_date & to_date) and coordinates (H3 https://h3geo.org/ that will be dynamically chosen from frontend) and do preaggregations ie. count of flights per hex in a given timeframe, latest coords of a specific fight etc.
5) to have a type-guaranteed, ideally ClickHouse-typegenerated GraphQL endpoint for frontend to query this data with based on live params (dates, h3 tiles, coordinate boundaries, flight types/details)
6) to render all of this GPU-accelerated in a single-page full screen react app with A) a bounded rectangle area around Finland B) pre-generated multiple resolutions of H3 hexagons to be rendered either empty from json or dynamically with data eg. with https://deck.gl/docs/api-reference/geo-layers/h3-hexagon-layer C) the dynamically filtered scraped flight data as scatterplot with deckgl https://deck.gl/docs/api-reference/layers/scatterplot-layer live (latest location per flight)
7) to have a fully automatic, scraper=>db=>graphql=>frontend working with subscriptions - if the scraper is configured to add more data once per minute, the frontend should update as soon as the data is processed through the pipeline and finally
8) to have the option of running all of these processes locally with process compose, and having the option of running nix test to make sure all per-package tests pass 

Can you make a new folder called "learnings" and store this as an 00_INITIAL_PROMPT.md and use this & the existing repo-code to generate a detailed 01_SPRINT_PLANS.md that will split these tasks into sprints, each one of which will have its own plan that is do be done later?